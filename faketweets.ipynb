{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jbajaj1/faketweets/blob/master/faketweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "di9uPiQKYhcT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SELMx44nZPLi"
   },
   "outputs": [],
   "source": [
    "def load_tweets(filename, initVoc=False):\n",
    "    X = []\n",
    "    y = []\n",
    "    r = open(filename, 'r')\n",
    "    for line in r:\n",
    "        line = line.split()\n",
    "        y.append(line[0])\n",
    "        X.append(line[1:])\n",
    "\n",
    "    tokenizedTweets = []\n",
    "    tokenizedLabels = []\n",
    "    labelDic = {\"negative\":0, \"neutral\":1, \"positive\":2}\n",
    "    if initVoc:\n",
    "        for t in X:\n",
    "            twitterVoc.add_sentence(t)\n",
    "    for l in y:\n",
    "        tokenizedLabels.append(labelDic[l])\n",
    "    for t in X:\n",
    "        tokenizedTweets.append(twitterVoc.sentence_to_vec(t))\n",
    "\n",
    "    tokenizedTweets = torch.LongTensor(tokenizedTweets)\n",
    "    tokenizedLabels = torch.LongTensor(tokenizedLabels)\n",
    "\n",
    "    return tokenizedTweets, tokenizedLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-mTUZvt6ZR-B"
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"UNK\"}\n",
    "        self.num_words = 2\n",
    "        self.num_sentences = 0\n",
    "        self.longest_sentence = 0\n",
    "        self.unknown_count = 0\n",
    "\n",
    "\n",
    "    def add_word(self, word):\n",
    "        word = word.lower()\n",
    "        if word not in self.word2index:\n",
    "            # First entry of word into vocabulary\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            # Word exists; increase word count\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        sentence_len = 0\n",
    "        #print(sentence)\n",
    "        for word in sentence:\n",
    "            sentence_len += 1\n",
    "            self.add_word(word)\n",
    "        if sentence_len > self.longest_sentence:\n",
    "            # This is the longest sentence\n",
    "            self.longest_sentence = sentence_len\n",
    "        # Count the number of sentences\n",
    "        self.num_sentences += 1\n",
    "\n",
    "    def to_word(self, index):\n",
    "        return self.index2word[index]\n",
    "\n",
    "    def to_index(self, word):\n",
    "        if word not in self.word2index:\n",
    "            #print(\"Unknown word:\", word)\n",
    "            self.unknown_count += 1\n",
    "            return 1\n",
    "        return self.word2index[word]\n",
    "\n",
    "    def sentence_to_vec(self, sentence):\n",
    "        vec = []\n",
    "        for word in sentence:\n",
    "            word = word.lower()\n",
    "            vec.append(self.to_index(word))\n",
    "        while len(vec) < self.longest_sentence:\n",
    "            vec.append(0)\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gSFkrFXsZcR0"
   },
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.rnn = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.output = nn.Linear(self.hidden_size, 3)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        emb = self.embedding(X)\n",
    "        emb = self.dropout(emb)\n",
    "\n",
    "        hidden_states, _ = self.rnn(emb)\n",
    "\n",
    "        numMask = (X != 0).float()\n",
    "        mask = (X != 0).float().unsqueeze(-1).expand(hidden_states.size())\n",
    "        hidden_states = (hidden_states*mask).sum(-2)/numMask.sum(-1).unsqueeze(-1)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        output_dist = self.output(hidden_states)\n",
    "\n",
    "        return output_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yo9i4p_QZjIk"
   },
   "outputs": [],
   "source": [
    "def validate(expected, predictions):\n",
    "    '''\n",
    "    totdiff = 0\n",
    "    numdiff = 0\n",
    "    counter = 0\n",
    "    for i in expected:\n",
    "        diff = abs(i-predictions[counter])\n",
    "        totdiff += diff\n",
    "        if diff != 0:\n",
    "            numdiff += 1\n",
    "        counter += 1\n",
    "    return totdiff, numdiff\n",
    "    '''\n",
    "    return confusion_matrix(expected, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "tPK-9ygJZkgr",
    "outputId": "603490b2-e839-4130-e62d-dce64c87285c"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7e817a633e38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtwitterVoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"twitter\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizedTweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizedLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../twitter_sentiment/semeval_train.txt\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0minitVoc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwitterVoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-dbe8f69dda3b>\u001b[0m in \u001b[0;36mload_tweets\u001b[0;34m(filename, initVoc)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../twitter_sentiment/semeval_train.txt'"
     ]
    }
   ],
   "source": [
    "twitterVoc = Vocab(\"twitter\")\n",
    "\n",
    "#Put proper location of file here\n",
    "tokenizedTweets, tokenizedLabels = load_tweets(\"../twitter_sentiment/semeval_train.txt\",  initVoc=True)\n",
    "\n",
    "print(twitterVoc.to_word(4))\n",
    "print(twitterVoc.to_index(\"this\"))\n",
    "\n",
    "print(twitterVoc.num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eBC48S6qa65d"
   },
   "outputs": [],
   "source": [
    "ourLSTM = LSTM(twitterVoc.num_words, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "thQWrw52bRS3"
   },
   "outputs": [],
   "source": [
    "##################\n",
    "#####Training#####\n",
    "##################\n",
    "\n",
    "opt = torch.optim.Adam(ourLSTM.parameters(), lr=.1)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "epochs = 2\n",
    "dataset = DataLoader(TensorDataset(tokenizedTweets, tokenizedLabels), batch_size=100)\n",
    "for i in range(epochs):\n",
    "    print(\"Training on epoch\", i)\n",
    "    for batchidx, (x, y) in enumerate(dataset):\n",
    "        opt.zero_grad()\n",
    "        outputs = ourLSTM(x)\n",
    "        lossVal = loss(outputs, y)\n",
    "        lossVal.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v9EKY6ihbUWf"
   },
   "outputs": [],
   "source": [
    "##################\n",
    "#####Predict######\n",
    "####Evaluation####\n",
    "##################\n",
    "\n",
    "\n",
    "predVal = ourLSTM(tokenizedTweets).argmax(dim=-1)\n",
    "\n",
    "print(\"Results for Train Data:\\n\", validate(tokenizedLabels, predVal))\n",
    "\n",
    "\n",
    "filelist = [\"Twitter2013_raw.txt\", \"Twitter2014_raw.txt\", \"Twitter2015_raw.txt\", \"Twitter2016_raw.txt\"]\n",
    "\n",
    "for file in filelist:\n",
    "    #Update file location here\n",
    "    tokTestTweets, tokTestLabels = load_tweets(\"../twitter_sentiment/\" + file)\n",
    "    predVal = ourLSTM(tokTestTweets).argmax(dim=-1)\n",
    "    print(\"Results for\", file, \"\\n\", validate(tokTestLabels, predVal))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Num Unknown Words:\", twitterVoc.unknown_count)\n",
    "#Test commit for github 2"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNllmUTa+5Wg84oeCaaTJK1",
   "include_colab_link": true,
   "name": "faketweets.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

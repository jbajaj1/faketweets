{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jbajaj1/faketweets/blob/master/faketweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "di9uPiQKYhcT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SELMx44nZPLi"
   },
   "outputs": [],
   "source": [
    "def load_tweets(filename, initVoc=False):\n",
    "    X = []\n",
    "    y = []\n",
    "    r = open(filename, 'r')\n",
    "    for line in r:\n",
    "        line = line.split()\n",
    "        y.append(line[0])\n",
    "        X.append(line[1:])\n",
    "\n",
    "    tokenizedTweets = []\n",
    "    tokenizedLabels = []\n",
    "    labelDic = {\"negative\":0, \"neutral\":1, \"positive\":2}\n",
    "    if initVoc:\n",
    "        for t in X:\n",
    "            twitterVoc.add_sentence(t)\n",
    "    for l in y:\n",
    "        tokenizedLabels.append(labelDic[l])\n",
    "    for t in X:\n",
    "        tokenizedTweets.append(twitterVoc.sentence_to_vec(t))\n",
    "\n",
    "    tokenizedTweets = torch.LongTensor(tokenizedTweets)\n",
    "    tokenizedLabels = torch.LongTensor(tokenizedLabels)\n",
    "\n",
    "    return tokenizedTweets, tokenizedLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-mTUZvt6ZR-B"
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"UNK\"}\n",
    "        self.num_words = 2\n",
    "        self.num_sentences = 0\n",
    "        self.longest_sentence = 0\n",
    "        self.unknown_count = 0\n",
    "\n",
    "\n",
    "    def add_word(self, word):\n",
    "        word = word.lower()\n",
    "        if word not in self.word2index:\n",
    "            # First entry of word into vocabulary\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            # Word exists; increase word count\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        sentence_len = 0\n",
    "        #print(sentence)\n",
    "        for word in sentence:\n",
    "            sentence_len += 1\n",
    "            self.add_word(word)\n",
    "        if sentence_len > self.longest_sentence:\n",
    "            # This is the longest sentence\n",
    "            self.longest_sentence = sentence_len\n",
    "        # Count the number of sentences\n",
    "        self.num_sentences += 1\n",
    "\n",
    "    def to_word(self, index):\n",
    "        return self.index2word[index]\n",
    "\n",
    "    def to_index(self, word):\n",
    "        if word not in self.word2index:\n",
    "            #print(\"Unknown word:\", word)\n",
    "            self.unknown_count += 1\n",
    "            return 1\n",
    "        return self.word2index[word]\n",
    "\n",
    "    def sentence_to_vec(self, sentence):\n",
    "        vec = []\n",
    "        for word in sentence:\n",
    "            word = word.lower()\n",
    "            vec.append(self.to_index(word))\n",
    "        while len(vec) < self.longest_sentence:\n",
    "            vec.append(0)\n",
    "        return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gSFkrFXsZcR0"
   },
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.rnn = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers, batch_first=True)\n",
    "        self.output = nn.Linear(self.hidden_size, 3)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        emb = self.embedding(X)\n",
    "        emb = self.dropout(emb)\n",
    "\n",
    "        hidden_states, _ = self.rnn(emb)\n",
    "\n",
    "        numMask = (X != 0).float()\n",
    "        mask = (X != 0).float().unsqueeze(-1).expand(hidden_states.size())\n",
    "        hidden_states = (hidden_states*mask).sum(-2)/numMask.sum(-1).unsqueeze(-1)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "\n",
    "        output_dist = self.output(hidden_states)\n",
    "\n",
    "        return output_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yo9i4p_QZjIk"
   },
   "outputs": [],
   "source": [
    "def validate(expected, predictions):\n",
    "    '''\n",
    "    totdiff = 0\n",
    "    numdiff = 0\n",
    "    counter = 0\n",
    "    for i in expected:\n",
    "        diff = abs(i-predictions[counter])\n",
    "        totdiff += diff\n",
    "        if diff != 0:\n",
    "            numdiff += 1\n",
    "        counter += 1\n",
    "    return totdiff, numdiff\n",
    "    '''\n",
    "    return confusion_matrix(expected, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "tPK-9ygJZkgr",
    "outputId": "603490b2-e839-4130-e62d-dce64c87285c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my\n",
      "82\n",
      "33059\n"
     ]
    }
   ],
   "source": [
    "twitterVoc = Vocab(\"twitter\")\n",
    "\n",
    "#Put proper location of file here\n",
    "tokenizedTweets, tokenizedLabels = load_tweets(\"../twitter_sentiment/semeval_train.txt\",  initVoc=True)\n",
    "\n",
    "print(twitterVoc.to_word(4))\n",
    "print(twitterVoc.to_index(\"this\"))\n",
    "\n",
    "print(twitterVoc.num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eBC48S6qa65d"
   },
   "outputs": [],
   "source": [
    "ourLSTM = LSTM(twitterVoc.num_words, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "thQWrw52bRS3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on epoch 0\n",
      "Training on epoch 1\n",
      "Training on epoch 2\n",
      "Training on epoch 3\n",
      "Training on epoch 4\n",
      "Training on epoch 5\n",
      "Training on epoch 6\n",
      "Training on epoch 7\n",
      "Training on epoch 8\n",
      "Training on epoch 9\n",
      "Training on epoch 10\n",
      "Training on epoch 11\n",
      "Training on epoch 12\n",
      "Training on epoch 13\n",
      "Training on epoch 14\n",
      "Training on epoch 15\n",
      "Training on epoch 16\n",
      "Training on epoch 17\n",
      "Training on epoch 18\n",
      "Training on epoch 19\n",
      "Training on epoch 20\n",
      "Training on epoch 21\n",
      "Training on epoch 22\n",
      "Training on epoch 23\n",
      "Training on epoch 24\n",
      "Training on epoch 25\n",
      "Training on epoch 26\n",
      "Training on epoch 27\n",
      "Training on epoch 28\n",
      "Training on epoch 29\n",
      "Training on epoch 30\n",
      "Training on epoch 31\n",
      "Training on epoch 32\n",
      "Training on epoch 33\n",
      "Training on epoch 34\n",
      "Training on epoch 35\n",
      "Training on epoch 36\n",
      "Training on epoch 37\n",
      "Training on epoch 38\n",
      "Training on epoch 39\n",
      "Training on epoch 40\n",
      "Training on epoch 41\n",
      "Training on epoch 42\n",
      "Training on epoch 43\n",
      "Training on epoch 44\n",
      "Training on epoch 45\n",
      "Training on epoch 46\n",
      "Training on epoch 47\n",
      "Training on epoch 48\n",
      "Training on epoch 49\n",
      "Training on epoch 50\n",
      "Training on epoch 51\n",
      "Training on epoch 52\n",
      "Training on epoch 53\n",
      "Training on epoch 54\n",
      "Training on epoch 55\n",
      "Training on epoch 56\n",
      "Training on epoch 57\n",
      "Training on epoch 58\n",
      "Training on epoch 59\n",
      "Training on epoch 60\n",
      "Training on epoch 61\n",
      "Training on epoch 62\n",
      "Training on epoch 63\n",
      "Training on epoch 64\n",
      "Training on epoch 65\n",
      "Training on epoch 66\n",
      "Training on epoch 67\n",
      "Training on epoch 68\n",
      "Training on epoch 69\n",
      "Training on epoch 70\n",
      "Training on epoch 71\n",
      "Training on epoch 72\n",
      "Training on epoch 73\n",
      "Training on epoch 74\n",
      "Training on epoch 75\n",
      "Training on epoch 76\n",
      "Training on epoch 77\n",
      "Training on epoch 78\n",
      "Training on epoch 79\n",
      "Training on epoch 80\n",
      "Training on epoch 81\n",
      "Training on epoch 82\n",
      "Training on epoch 83\n",
      "Training on epoch 84\n",
      "Training on epoch 85\n",
      "Training on epoch 86\n",
      "Training on epoch 87\n",
      "Training on epoch 88\n",
      "Training on epoch 89\n",
      "Training on epoch 90\n",
      "Training on epoch 91\n",
      "Training on epoch 92\n",
      "Training on epoch 93\n",
      "Training on epoch 94\n",
      "Training on epoch 95\n",
      "Training on epoch 96\n",
      "Training on epoch 97\n",
      "Training on epoch 98\n",
      "Training on epoch 99\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#####Training#####\n",
    "##################\n",
    "\n",
    "opt = torch.optim.Adam(ourLSTM.parameters(), lr=.1)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "epochs = 100\n",
    "dataset = DataLoader(TensorDataset(tokenizedTweets, tokenizedLabels), batch_size=100)\n",
    "for i in range(epochs):\n",
    "    print(\"Training on epoch\", i)\n",
    "    for batchidx, (x, y) in enumerate(dataset):\n",
    "        opt.zero_grad()\n",
    "        outputs = ourLSTM(x)\n",
    "        lossVal = loss(outputs, y)\n",
    "        lossVal.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v9EKY6ihbUWf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Train Data:\n",
      " [[1225   17   23]\n",
      " [  29 4015   65]\n",
      " [  16   15 3199]]\n",
      "Results for Twitter2013_raw.txt \n",
      " [[125 243 233]\n",
      " [117 897 625]\n",
      " [110 495 967]]\n",
      "Results for Twitter2014_raw.txt \n",
      " [[ 52  78  72]\n",
      " [ 49 348 272]\n",
      " [ 54 306 621]]\n",
      "Results for Twitter2015_raw.txt \n",
      " [[ 89 128 147]\n",
      " [ 98 527 362]\n",
      " [ 85 342 611]]\n",
      "Results for Twitter2016_raw.txt \n",
      " [[ 770 1237 1224]\n",
      " [1207 5109 4025]\n",
      " [ 545 2427 4087]]\n",
      "Num Unknown Words: 104225\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "#####Predict######\n",
    "####Evaluation####\n",
    "##################\n",
    "\n",
    "\n",
    "predVal = ourLSTM(tokenizedTweets).argmax(dim=-1)\n",
    "\n",
    "print(\"Results for Train Data:\\n\", validate(tokenizedLabels, predVal))\n",
    "\n",
    "\n",
    "filelist = [\"Twitter2013_raw.txt\", \"Twitter2014_raw.txt\", \"Twitter2015_raw.txt\", \"Twitter2016_raw.txt\"]\n",
    "\n",
    "for file in filelist:\n",
    "    #Update file location here\n",
    "    tokTestTweets, tokTestLabels = load_tweets(\"../twitter_sentiment/\" + file)\n",
    "    predVal = ourLSTM(tokTestTweets).argmax(dim=-1)\n",
    "    print(\"Results for\", file, \"\\n\", validate(tokTestLabels, predVal))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Num Unknown Words:\", twitterVoc.unknown_count)\n",
    "#Test commit for github 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNllmUTa+5Wg84oeCaaTJK1",
   "include_colab_link": true,
   "name": "faketweets.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
